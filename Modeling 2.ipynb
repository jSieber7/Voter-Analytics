{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# GSS Data Analysis: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Part 1: Cleaning the data\n",
    "\n",
    "The data currently comes in an all numeric format, making visuals and understanding the data difficult. Additionaly, all NAs seem like actual values. Cleaning the data is the first step in analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step One: Converting the data to and from Strings\n",
    "\n",
    "We can use the metadata included with the data download to easily remap strings to numbers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from function_package import (recur_dictify, DataFrameSelector, BinaryEncoding, DataFrameSelector,\n",
    "                     UniqueThreshold, FilterNAs, DataFrameSelectorAndRecode, QuickModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the data\n",
    "\n",
    "df = pd.read_csv('data/GSS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the meta data as a tool for converting the numeric values of the df into strings and then back\n",
    "\n",
    "df_remap  = pd.read_excel('data/GSS_metadata.xlsx',sheet_name='Codes')\n",
    "df_remap = df_remap.iloc[:,1:]\n",
    "df_remap = df_remap.fillna(method='ffill')\n",
    "df_remap['Variable Name'] = df_remap['Variable Name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataframe will help turn the strings back into numbers\n",
    "# This will help speed up recoding ordinal values for machine learning\n",
    "\n",
    "df_remap_reverse = df_remap[['Variable Name','Label','Code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaper = recur_dictify(df_remap)\n",
    "remaper_reverse = recur_dictify(df_remap_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the numbers to strings and drops NAs\n",
    "\n",
    "df_strings = df.replace(remaper)\n",
    "df_strings = df_strings.replace([\"No answer\",\"Don't know\",\"Not applicable\",\"-1\"],np.NaN)\n",
    "# df_strings.to_csv('data/data_strings.csv', index=False)\n",
    "df_strings = pd.read_csv('data/data_strings.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must be more specific to convert strings back into numbers\n",
    "# This is because the pandas 'replace' function fails for the 'remaper_reverse' dictionary\n",
    "\n",
    "df = df_strings\n",
    "for key in remaper_reverse.keys():\n",
    "    df[key] = df_strings[key].map(remaper_reverse[key]).fillna(df_strings[key])\n",
    "\n",
    "# df.to_csv('data/data_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step Two: Determining what is categorical/ordinal\n",
    "\n",
    "__**Current Assumptions**__\n",
    "\n",
    "* Ordinals are scaled, assumed to be continous. Every column that has more than 11 distinct values (with many execptions) is considered to be ordinal/continous\n",
    "\n",
    "__**Gotchas**__\n",
    "\n",
    "* helpful,fair,trust,age (considered ordinal) - 3 to 2\n",
    "    * Image from metadata explains the problem: ![3 to 2](data/img/2-3_Switch.png)\n",
    "* Premarsx, teensex, xmarsex, homosex, bible - 'other' trouble\n",
    "    * 'other' appears to not be entered within the data after 2000\n",
    "* Nominal attributes under 11 - spjewaj, sprel?, relig16?, denom16?, relig?, parborn, race, sector, dipged, pawrkslf, spwrksta, marital, wrkstat, region\n",
    "    * presidental - i.e. vote12,pres12,if12who,...\n",
    "* tax - R pays none<vol.>/4\n",
    "    * 'R pays none<vol.>/4' appears to not be entered within the data after 2000\n",
    "* ordinal w/ large amount of categories - rincom06,rincome,income\n",
    "* Drop: ID, dateinv, cohort: these are primarly for record keeping purposes\n",
    "* 'SPOTHER','OTH16','OTHER' appear to strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the column names of the Nominal, Ordinal (coded as Continuous), and Binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We operate under the tested assumption that unless specified or over 11 distinct values,\n",
    "# the column is ordinal.\n",
    "\n",
    "# We end up with four distinct lists of columns that we will use in our data pipeline:\n",
    "# Ordinal, Nominal, Binary, and Drop\n",
    "\n",
    "# Note: We in fact will not use the Drop list in the pipelines, as we will just not select these columns\n",
    "\n",
    "\n",
    "def GetColumnNames(df, Target_Variables = ['POLVIEWS']):\n",
    "    '''Simply uses our dataframe and makes a list of the nominal, ordinal, and binary columns.'''\n",
    "    \n",
    "    # Gets the dataframe column names alongisde the number of unique entries\n",
    "    # Forms the basis of our rule system: if over 11 distinct categories then it is nominal\n",
    "    df_columns = df.nunique()\n",
    "\n",
    "    # Columns that may become exceptions to the rules\n",
    "    Drop = ['ID_','DATEINTV','COHORT','SPOTHER','OTH16','OTHER']\n",
    "    Continuous = [\"YEAR\",\"HRS1\",\"HRS2\",\"AGEWED\",\"SPHRS1\",\"SIBS\",\"CHILDS\",\"AGE\",\"WHENHS\",\"WHENCOL\",\n",
    "             \"HOMPOP\",\"CHLDIDEL\",\"COHORT\",\"BABIES\",\"PRETEEN\",\"TEENS\",\"TVHOURS\",'WEEKSWRK']\n",
    "    Small_Categorical = [\"REGION\",\"WRKSTAT\",\"MARITAL\",'SPWRKSTA','PAWRKSLF','DIPGED','SECTOR','RACE','PARBORN',\n",
    "                        'PRES00','VOTE04','PRES04','IF04WHO','VOTE00','VOTE08','PRES08',\n",
    "                        'IF08WHO','VOTE12','PRES12','IF12WHO']\n",
    "    Large_Ordinal = ['RINCOM06','RINCOME','INCOME']\n",
    "\n",
    "    # Gets rid of all of the columns manually specified\n",
    "    df_columns = df_columns[~(df_columns.index.isin(Drop)) &\n",
    "                       ~(df_columns.index.isin(Continuous)) &\n",
    "                       ~(df_columns.index.isin(Small_Categorical)) &\n",
    "                       ~(df_columns.index.isin(Large_Ordinal)) &\n",
    "                       ~(df_columns.index.isin(Target_Variables))]\n",
    "\n",
    "    # Now we make the lists of columns one by one\n",
    "    Nominal = list(df_columns[df_columns >= 11].index & df_columns[df_columns != 2].index)\n",
    "    Nominal = Nominal + Small_Categorical\n",
    "    # Checks to see if all of the list items are in the column index of the Df. If not, drop them.\n",
    "    Nominal = [col for col in Nominal if col in df.columns]\n",
    "    \n",
    "    Ordinal = list(df_columns[df_columns <= 11].index & df_columns[df_columns != 2].index) \n",
    "    Ordinal = Ordinal + Continuous + Large_Ordinal\n",
    "    Ordinal = [col for col in Ordinal if col in df.columns]\n",
    "    \n",
    "    Binary = list(df_columns[df_columns == 2].index)\n",
    "    Binary = [col for col in Binary if col in df.columns]\n",
    "    \n",
    "    return Nominal, Ordinal, Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_clean.csv',low_memory=False)\n",
    "\n",
    "# Uses our freshly made function to get column names\n",
    "Nominal, Ordinal, Binary = GetColumnNames(df)\n",
    "\n",
    "# Replacing 2 with 3\n",
    "change_cols = ['HELPFUL','FAIR','TRUST','AGE']\n",
    "df[change_cols] = df[change_cols].replace(3,786)\n",
    "df[change_cols] = df[change_cols].replace(2,3)\n",
    "df[change_cols] = df[change_cols].replace(786,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step Three: Pipelines\n",
    "\n",
    "In order to automate the preprocessing steps, we will use sklearn's pipelines. Not only does this lead to less errors, but we can go back and make modifications as necessary. Note, here we automatically get rid of records that are before 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's only include enteries after 1999 and where POLVIEWS is not NA\n",
    "df_filtered = df[(df['YEAR'] >= 2000) & ~(np.isnan(df['POLVIEWS']))]\n",
    "\n",
    "# Second let's use the polviews as the primary variable\n",
    "X_df = df_filtered.drop('POLVIEWS', axis = 1)\n",
    "y = df_filtered['POLVIEWS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make a small pipeline to implement changes\n",
    "df_pipeline = Pipeline([\n",
    "    ('drop_categoricals', UniqueThreshold(threshold = 10, ignore_columns=Ordinal)),\n",
    "    \n",
    "    # Note: Threshold means that all columns with a NaN ratio of below 'threshold' will be kept\n",
    "    ('drop_NAs', FilterNAs(threshold = .8))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the DataFrame Pipeline\n",
    "X = df_pipeline.fit_transform(X_df)\n",
    "\n",
    "# Get new column names\n",
    "Nominal, Ordinal, Binary = GetColumnNames(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Array Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the way pipelines are implemented, we must respecify our pipelines if an arguement changes\n",
    "\n",
    "ordinal_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(Ordinal)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "nominal_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelectorAndRecode(Nominal,remaper)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "    ('encode', OneHotEncoder(categories='auto'))\n",
    "])\n",
    "\n",
    "binary_pipeline = Pipeline([\n",
    "     ('selector', DataFrameSelector(Binary)),\n",
    "     ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "     ('binary', BinaryEncoding())\n",
    "])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    ('nom_pipe', nominal_pipeline),\n",
    "    (\"ord_pipe\", ordinal_pipeline),\n",
    "    ('bin_pipe', binary_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we put it through the Numpy Array Pipeline\n",
    "X = full_pipeline.fit_transform(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20602, 270)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The output of a pipeline is a Numpy array, which cannot have column names.\n",
    "# Now that we confirmed the dataframe is of an acceptable size, we can turn the Numpy array back into a DataFrame.\n",
    "\n",
    "ordinal_col_names = list(np.asarray(Ordinal))\n",
    "nominal_col_names = list(np.hstack((nominal_pipeline.named_steps['encode'].categories_)))\n",
    "binary_col_names = list(np.asarray(Binary))\n",
    "attributes = np.concatenate([nominal_col_names,ordinal_col_names,binary_col_names])\n",
    "X = pd.DataFrame(X.toarray(),columns=attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    16291\n",
       "0.0     4311\n",
       "Name: FEAR, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After imputation\n",
    "\n",
    "X.FEAR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    8315\n",
       "1.0    4311\n",
       "Name: FEAR, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before imputation\n",
    "\n",
    "df_filtered.FEAR.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Dummy Variable Reference Category\n",
    "\n",
    "When we use Least Sqaures or Maximum Likelihood Estimation, we must leave a reference category. Other machine learning models can keep this reference category. Therefore, we will make a special X set with a reference category for our linear modeling with a slight modification to our pipelines. Note, while it seems as though Sklearn does not have an easy dummy encoding w/ reference, Pandas has an easy function we can apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we get a list of all of the unqiue categories\n",
    "# In this manner we can backtrack our dummy encoding and drop a column\n",
    "# In this case, we drop the last dummy encoded column as a reference\n",
    "\n",
    "ref = X_df[Nominal]\n",
    "ref = ref.fillna(method='ffill')\n",
    "ref = ref.fillna(method='bfill')\n",
    "ref = ref.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a list of the indexed columns to drop thru a running sum of distinct values\n",
    "# Note we start at -1, as indexing starts a zero\n",
    "\n",
    "running_sum = -1\n",
    "drop_list = []\n",
    "\n",
    "for i in ref.values:\n",
    "    running_sum += i\n",
    "    drop_list.append(running_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference categories are: {'REGION': 'W. sou. central', 'WRKSTAT': 'Working parttime', 'MARITAL': 'Widowed', 'SPWRKSTA': 'Working parttime', 'PAWRKSLF': 'Someone else', 'DIPGED': 'Other', 'RACE': 'White', 'PARBORN': 'Not mother;fa.dk', 'VOTE04': 'Voted', 'PRES04': 'Nader', 'VOTE00': 'Voted', 'VOTE08': 'Voted', 'PRES08': 'Other candidate (specify)', 'VOTE12': 'Voted'}\n"
     ]
    }
   ],
   "source": [
    "reference_categories = X.iloc[:,drop_list].columns\n",
    "X_reference = X.drop(reference_categories, axis = 1)\n",
    "reference_categories_labels = {i[0]: i[1] for i in zip(Nominal, reference_categories.values)}\n",
    "\n",
    "print('Reference categories are: {}'.format(reference_categories_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Outcomes\n",
    "\n",
    "Now our data is fully clean and ready for machine learning algorithms, however we should be careful of the assumptions that we have made and the final shape of the data.\n",
    "\n",
    "Note that we have made a large amount of simple imputations, such as FEAR: ![Imputation](data/img/Imputation.png)\n",
    "\n",
    "This may be adverse in the long-run, as we know that assuming all of those who did not answer choose the most popular option will not have good accuracy. We may have to be more selective in the columns we keep or use more complex imputation methods in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Part 2: Modeling\n",
    "\n",
    "First, we will try to predict POLVIEWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Quick Fitting Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use statsmodels to get the p-values\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Lars, Ridge, Lasso, ElasticNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step One: Linear Models\n",
    "\n",
    "Linear models are some of the most simple and interpretable models. We start with pure linear regression. Given that we have over 300 variables, we know that many will not be correlated with POLVIEWS. Linear regression tends to be greedy in variable selection, so we used more generalized models for variable selection. \n",
    "\n",
    "After linear regression, we do least angle regression, which is very similar stepwise regression. Then, we use Ridge, Lasso, and ElasticNet in order to lower the amount of 'noise' variables that our basic linear model has. Note that we will use cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Linear Regrssion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must reset our index, or the linear model will fail\n",
    "X_reference = X_reference.reset_index(drop=True)\n",
    "y_reference = y.reset_index(drop=True)\n",
    "\n",
    "# We must add a constant manually, as the linear model will not add one automatically\n",
    "X_model = sm.add_constant(X_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Sqaured: 0.36075096584375166\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y_reference,X_model)\n",
    "model = model.fit()\n",
    "# model.summary() Long Output\n",
    "print('R-Sqaured:', model.rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beta</th>\n",
       "      <th>P-Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PARTYID</th>\n",
       "      <td>0.355964</td>\n",
       "      <td>1.318975e-264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>3.082136</td>\n",
       "      <td>1.739679e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EQWLTH</th>\n",
       "      <td>0.085234</td>\n",
       "      <td>7.075301e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAPPUN</th>\n",
       "      <td>-0.137560</td>\n",
       "      <td>6.083830e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRASS</th>\n",
       "      <td>0.152513</td>\n",
       "      <td>2.512404e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOMOSEX</th>\n",
       "      <td>-0.076947</td>\n",
       "      <td>3.324028e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATARMS</th>\n",
       "      <td>-0.056600</td>\n",
       "      <td>1.694503e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONLABOR</th>\n",
       "      <td>0.057868</td>\n",
       "      <td>1.931208e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black</th>\n",
       "      <td>0.175395</td>\n",
       "      <td>6.915570e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PILLOK</th>\n",
       "      <td>0.047514</td>\n",
       "      <td>2.557423e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>-0.044327</td>\n",
       "      <td>1.926125e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATCRIMY</th>\n",
       "      <td>-0.046570</td>\n",
       "      <td>2.662658e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WRKWAYUP</th>\n",
       "      <td>-0.050807</td>\n",
       "      <td>6.221347e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RELITEN</th>\n",
       "      <td>-0.046455</td>\n",
       "      <td>1.119004e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PORNLAW</th>\n",
       "      <td>-0.041075</td>\n",
       "      <td>1.137524e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATENVIY</th>\n",
       "      <td>0.044013</td>\n",
       "      <td>1.248902e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONARMY</th>\n",
       "      <td>-0.044269</td>\n",
       "      <td>1.458076e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONPRESS</th>\n",
       "      <td>0.040386</td>\n",
       "      <td>2.725853e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEXEDUC</th>\n",
       "      <td>0.158935</td>\n",
       "      <td>2.882922e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATARMSY</th>\n",
       "      <td>-0.036427</td>\n",
       "      <td>3.287843e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Beta        P-Value\n",
       "PARTYID   0.355964  1.318975e-264\n",
       "const     3.082136   1.739679e-15\n",
       "EQWLTH    0.085234   7.075301e-13\n",
       "CAPPUN   -0.137560   6.083830e-12\n",
       "GRASS     0.152513   2.512404e-11\n",
       "HOMOSEX  -0.076947   3.324028e-11\n",
       "NATARMS  -0.056600   1.694503e-10\n",
       "CONLABOR  0.057868   1.931208e-10\n",
       "Black     0.175395   6.915570e-09\n",
       "PILLOK    0.047514   2.557423e-07\n",
       "TAX      -0.044327   1.926125e-06\n",
       "NATCRIMY -0.046570   2.662658e-06\n",
       "WRKWAYUP -0.050807   6.221347e-06\n",
       "RELITEN  -0.046455   1.119004e-05\n",
       "PORNLAW  -0.041075   1.137524e-05\n",
       "NATENVIY  0.044013   1.248902e-05\n",
       "CONARMY  -0.044269   1.458076e-05\n",
       "CONPRESS  0.040386   2.725853e-05\n",
       "SEXEDUC   0.158935   2.882922e-05\n",
       "NATARMSY -0.036427   3.287843e-05"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_summary = pd.DataFrame({'Beta':model.params,'P-Value':model.pvalues})\n",
    "model_summary.sort_values('P-Value', inplace=True)\n",
    "\n",
    "# The lowest 20 variables in terms of p-value.\n",
    "model_summary[model_summary['P-Value'] <= .05][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(model.resid, bins=30)\n",
    "ax.set_title('Error Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(model.fittedvalues,model.resid)\n",
    "ax.set_xlabel('Fitted')\n",
    "ax.set_ylabel('Residual')\n",
    "ax.set_title('Residuals vs Fitted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Overview\n",
    "\n",
    "The R-Sqaured for the linear model is .364 on the training data. \n",
    "\n",
    "While our linear model does tell us some interesting patterns, we should take a look at the assumptions that were violated in the model. We can see that our error is normally distributed with a mean of zero, but we do not have homoscedasticity in our errors. There appears to be systematic bias, and the relationship between the target variable and the predictors is likely not linear. Also, one could also say that using the 'PARTYID' variable should be removed when predicting due to the innate correlianity with 'POLVIEWS'. The bias found in the Residuals vs Fitted chart also calls into question whether we should use the target variable as a continuous variable, or change it to categorical. We can experiment with different models later.\n",
    "\n",
    "Looking past the violated assumptions, we can generate some hypotheses about the data. While we will not have a lot of confidence with these hypotheses, they can set the baseline for hypotheses in the future. The main hypothesis is that: While holding everything else equal the following variables are significant predictors of a person's political status:\n",
    "* WRKSLF (Respondent self-emp or works for somebody)\n",
    "* RACE (Black, White, or Other)\n",
    "* MARITAL (Marital Status)\n",
    "* EQUAL (Should government try to equalize wealth gap?)\n",
    "* CAPPUN (Favor or Oppose death penalty)\n",
    "* CONLABOR (Does respondent have confidence in the people in the leaders of industry)\n",
    "\n",
    "In terms of tying this into our problem of advising a politician during a run for office, we could derive great value from proving this hypothesis. We could see that a liberal type of candidate could find great support with other liberals if she/he strong opposes income inequality and and the death penalty at the cost of losing popularity with moderates and conservatives. If one were running a presidential campiagn, one could emphasize these typically liberal views during the run to get onto the presidential ticket and then shift to more moderate views after to appease the moderates. One can also use these relationships as a tool to segregate liberals, conservatives, and moderates in targeted advertising campaigns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### More Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Modeling with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lars Cross Validation: [0.01742255 0.01809097 0.02028363 0.02431995 0.02382228]\n",
      "Ridge Cross Validation: [0.24624823 0.31735218 0.32442218 0.37727894 0.3886654 ]\n",
      "Lasso Cross Validation: [-1.88579591e-06 -3.26802226e-07 -5.57022686e-04 -4.38629753e-04\n",
      " -1.17178957e-05]\n",
      "ElasticNet Cross Validation: [0.0460751  0.04764606 0.04504328 0.04437528 0.04342092]\n"
     ]
    }
   ],
   "source": [
    "# Now we can test out the more advanced regression models\n",
    "\n",
    "test_models = [Lars(eps=15), Ridge(), Lasso(max_iter=15), ElasticNet()]\n",
    "test_model_names = ['Lars','Ridge','Lasso','ElasticNet']\n",
    "QuickModel(test_models,test_model_names,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On models with defaults, the best model in terms of Rsqaured is ridge regression. On some folds however, it performs worse than the regular regression. Let's experiment with the penalty of the ridge,elasticc, and lasso of the regression in order to see if we can get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# L1 Penality (ie error + alpha * sum of all betas)\n",
    "lasso_params = {'alpha': [.005,.3,.5, 1, 5, 10, 15, 50]}\n",
    "\n",
    "# L2 Penality (ie error + alpha * sum of all betas^2)\n",
    "ridge_params = {'alpha': [10]}\n",
    " \n",
    "\n",
    "elastic_params = {\n",
    "    'alpha': [.5, 1,15, 50,100,200,300,400],\n",
    "    'l1_ratio' : [.7,.5,.3,.2,.1,.05]\n",
    "}\n",
    "\n",
    "# Takes a while to run. I elect to dump to a pickle and load back out to save time.\n",
    "ridge_search = GridSearchCV(Ridge(),ridge_params,cv=5).fit(X,y)\n",
    "lasso_search = GridSearchCV(Lasso(),lasso_params,cv=5).fit(X,y)\n",
    "# elastic_search = GridSearchCV(ElasticNet(),elastic_params,cv=5).fit(X,y)\n",
    "\n",
    "# joblib.dump(ridge_search, \"data/models/ridge_model.pkl\")\n",
    "# joblib.dump(lasso_search, \"data/models/lasso_model.pkl\")\n",
    "# joblib.dump(elastic_search, \"data/models/elastic_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lass = LassoCV(cv=5).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = RidgeCV(cv=5).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005053011025422339"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lass.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3607506642748505"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,\n",
       "    max_iter=1000, n_alphas=100, n_jobs=None, normalize=False,\n",
       "    positive=False, precompute='auto', random_state=None,\n",
       "    selection='cyclic', tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LassoCV(cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_search = joblib.load(\"data/models/ridge_model.pkl\")\n",
    "lasso_search = joblib.load('data/models/lasso_model.pkl')\n",
    "elastic_search = joblib.load('data/models/elastic_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: {'alpha': 500} 0.3428002734775513\n",
      "Lasso: {'alpha': 0.3} 0.1589181023816902\n",
      "Elastic: {'alpha': 0.5, 'l1_ratio': 0.05} 0.3169783173969842\n"
     ]
    }
   ],
   "source": [
    "print('Ridge:',ridge_search.best_params_,ridge_search.best_score_)\n",
    "print('Lasso:',lasso_search.best_params_,lasso_search.best_score_)\n",
    "print('Elastic:',elastic_search.best_params_,elastic_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the testing clearly show that Ridge regression is far superior to Lasso, and we need a very high penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to file\n",
    "# X.to_csv('data/training/X.csv',index=False)\n",
    "# y.to_csv('data/training/y.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Step 2: Decision Tree Techniques and other Non-Linear Models\n",
    "\n",
    "Decision Tree methods will be useful for not only for prediction, but variable importance and high interpretability will be pretty useful. We can use information based on the assumption that Y is ordinal and Y is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (GradientBoostingClassifier, GradientBoostingRegressor, \n",
    "RandomForestClassifier, RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = OneHotEncoder(categories='auto').fit_transform(y.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dec Tree Cross Validation: [-0.46007707 -0.28382405 -0.32198235 -0.37053    -0.27394761]\n",
      "Grad Boost Cross Validation: [0.28181314 0.34833049 0.35348339 0.39829998 0.40990429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakes\\AppData\\Local\\Continuum\\anaconda3\\envs\\voters\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross Validation: [0.2063234  0.30081959 0.2897287  0.33764601 0.34857385]\n"
     ]
    }
   ],
   "source": [
    "test_models = [DecisionTreeRegressor(), GradientBoostingRegressor(), RandomForestRegressor()]\n",
    "test_model_names = ['Dec Tree','Grad Boost','Random Forest']\n",
    "QuickModel(test_models,test_model_names,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dec Tree Cross Validation: [-0.41019059 -0.28504071 -0.33487804 -0.35022585 -0.27214459]\n",
      "Grad Boost Cross Validation: [0.28181314 0.34831261 0.35359246 0.39850338 0.40990429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakes\\AppData\\Local\\Continuum\\anaconda3\\envs\\voters\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross Validation: [0.18684415 0.29967228 0.28893637 0.32779267 0.35065183]\n"
     ]
    }
   ],
   "source": [
    "test_models = [DecisionTreeRegressor(), GradientBoostingRegressor(), RandomForestRegressor()]\n",
    "test_model_names = ['Dec Tree','Grad Boost','Random Forest']\n",
    "QuickModel(test_models,test_model_names,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model = model.fit(X,y)\n",
    "\n",
    "\n",
    "# feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(model.feature_importances_,X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PARTYID</th>\n",
       "      <td>0.528815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEGREE</th>\n",
       "      <td>0.033931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOMOSEX</th>\n",
       "      <td>0.028136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIBLE</th>\n",
       "      <td>0.027360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EQWLTH</th>\n",
       "      <td>0.024764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ATTEND</th>\n",
       "      <td>0.020856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FUND</th>\n",
       "      <td>0.018851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOD</th>\n",
       "      <td>0.015053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRASS</th>\n",
       "      <td>0.012779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAPPUN</th>\n",
       "      <td>0.012369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HELPSICK</th>\n",
       "      <td>0.011579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RELITEN</th>\n",
       "      <td>0.009841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HELPNOT</th>\n",
       "      <td>0.009447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEXEDUC</th>\n",
       "      <td>0.009422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABRAPE</th>\n",
       "      <td>0.009331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEENSEX</th>\n",
       "      <td>0.009288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WRKWAYUP</th>\n",
       "      <td>0.008899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PREMARSX</th>\n",
       "      <td>0.008645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FEFAM</th>\n",
       "      <td>0.008416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PILLOK</th>\n",
       "      <td>0.008207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHILDS</th>\n",
       "      <td>0.007831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HELPPOOR</th>\n",
       "      <td>0.007541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABSINGLE</th>\n",
       "      <td>0.007438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONLABOR</th>\n",
       "      <td>0.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kerry</th>\n",
       "      <td>0.006194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PORNLAW</th>\n",
       "      <td>0.006104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RELPERSN</th>\n",
       "      <td>0.005637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATARMS</th>\n",
       "      <td>0.005339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABDEFECT</th>\n",
       "      <td>0.005144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATENVIR</th>\n",
       "      <td>0.004893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Didnt vote</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hs diploma after post hs classes</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High school diploma</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEENS</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Did not vote</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Refused to answer</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retired</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHLDIDEL</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOMPOP</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>School</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Temp not working</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unempl, laid off</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPHRS1</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRAY</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FUND16</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BALLOT</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAPPY</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAPMAR</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEALTH</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Did not vote</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HELPFUL</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAIR</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FEJOBAFF</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DISCAFF</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nader</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self-employed</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Someone else</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ged</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E. nor. central</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         0\n",
       "PARTYID                           0.528815\n",
       "DEGREE                            0.033931\n",
       "HOMOSEX                           0.028136\n",
       "BIBLE                             0.027360\n",
       "EQWLTH                            0.024764\n",
       "ATTEND                            0.020856\n",
       "FUND                              0.018851\n",
       "GOD                               0.015053\n",
       "GRASS                             0.012779\n",
       "CAPPUN                            0.012369\n",
       "HELPSICK                          0.011579\n",
       "RELITEN                           0.009841\n",
       "HELPNOT                           0.009447\n",
       "SEXEDUC                           0.009422\n",
       "ABRAPE                            0.009331\n",
       "TEENSEX                           0.009288\n",
       "WRKWAYUP                          0.008899\n",
       "PREMARSX                          0.008645\n",
       "FEFAM                             0.008416\n",
       "PILLOK                            0.008207\n",
       "CHILDS                            0.007831\n",
       "HELPPOOR                          0.007541\n",
       "ABSINGLE                          0.007438\n",
       "CONLABOR                          0.006200\n",
       "Kerry                             0.006194\n",
       "PORNLAW                           0.006104\n",
       "RELPERSN                          0.005637\n",
       "NATARMS                           0.005339\n",
       "ABDEFECT                          0.005144\n",
       "NATENVIR                          0.004893\n",
       "...                                    ...\n",
       "Didnt vote                        0.000000\n",
       "Other                             0.000000\n",
       "Hs diploma after post hs classes  0.000000\n",
       "High school diploma               0.000000\n",
       "TEENS                             0.000000\n",
       "Did not vote                      0.000000\n",
       "Refused to answer                 0.000000\n",
       "Retired                           0.000000\n",
       "CHLDIDEL                          0.000000\n",
       "HOMPOP                            0.000000\n",
       "School                            0.000000\n",
       "Temp not working                  0.000000\n",
       "Unempl, laid off                  0.000000\n",
       "SPHRS1                            0.000000\n",
       "PRAY                              0.000000\n",
       "FUND16                            0.000000\n",
       "BALLOT                            0.000000\n",
       "HAPPY                             0.000000\n",
       "HAPMAR                            0.000000\n",
       "HEALTH                            0.000000\n",
       "Did not vote                      0.000000\n",
       "HELPFUL                           0.000000\n",
       "FAIR                              0.000000\n",
       "FEJOBAFF                          0.000000\n",
       "DISCAFF                           0.000000\n",
       "Nader                             0.000000\n",
       "Self-employed                     0.000000\n",
       "Someone else                      0.000000\n",
       "Ged                               0.000000\n",
       "E. nor. central                   0.000000\n",
       "\n",
       "[270 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance.sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def QuickModel(ml_algo,names,X,y):\n",
    "    '''Quick modeling and cross validation for multiple models'''\n",
    "    for algo,name in zip(ml_algo,names):\n",
    "        Model = algo\n",
    "        Model = Model.fit(X,y)\n",
    "        print(name, 'Cross Validation:', cross_val_score(Model,X,y,cv=5,scoring='r2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.neighbors.KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clustering and Profile Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-48-718caf396770>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-48-718caf396770>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    cluster_variables =\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cluster_variables = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (voters)",
   "language": "python",
   "name": "voters"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
